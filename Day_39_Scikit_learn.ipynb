{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPP2E0A9wc/cMD9Unr3xmCp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Scikit learn, a library which is written in Python and built upon Scipy, Matplotlib and Numpy provides a set of useful and efficient tools for machine learning and statistical modeling including regression, classification, clustering, predictive data analysis and dimensionality reduction etc and known as the most robust and useful library for Machine Learning."
      ],
      "metadata": {
        "id": "WRxTQDQ4IVtc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlIpJeosIHfY",
        "outputId": "7c8cb49d-5924-4196-e6e5-ec5398facef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegressionCV"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Data"
      ],
      "metadata": {
        "id": "bnxDshvOKd2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv('https://raw.githubusercontent.com/thunderstroke325/60-Days-of-Data-Science-and-ML/main/datasets/imdb_data.csv')\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umTDG_zzKf_B",
        "outputId": "a54d9ebd-0cad-4c67-c4b8-98b12e8b0780"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transforming Documents into Feature Vectors"
      ],
      "metadata": {
        "id": "VD5Iz1U7MJAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = CountVectorizer()\n",
        "docs = np.array (['The sun is shining',\n",
        "'The weather is sweet',\n",
        "'The sun is shining, the weather is sweet, and one and one is two'])\n",
        "bag = count.fit_transform(docs)\n",
        "print(bag.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVp3BF44MOAI",
        "outputId": "7efec55a-d53b-4abe-9c29-b839522cc900"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 1 1 0 1 0 0]\n",
            " [0 1 0 0 0 1 1 0 1]\n",
            " [2 3 2 1 1 1 2 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(count.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpWwCTa_MVM1",
        "outputId": "668f7038-d30d-4aa5-fff3-a377e56555be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Term Frequency-Inverse Document Frequency"
      ],
      "metadata": {
        "id": "10KUbvqbMaWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision = 2)\n",
        "tfidf = TfidfTransformer(use_idf = True, norm ='l2',smooth_idf = True)"
      ],
      "metadata": {
        "id": "rV7MXPxnMf40"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "YpbnD3ByMj3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
        "        ' '.join(emoticons).replace('-', '')\n",
        "    return text\n",
        "preprocessor(df.loc[0,'review'][-50:])\n",
        "df['review'] = df['review'].apply(preprocessor)"
      ],
      "metadata": {
        "id": "jMK7-Zg3Mlv6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Tokenization"
      ],
      "metadata": {
        "id": "2CSkUN5BMpTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]\n",
        "print(tokenizer('Its a wonderful day'))\n",
        "print(tokenizer_porter('Beautiful day'))\n",
        "stop = stopwords.words('english')\n",
        "[w for w in tokenizer_porter('Snow feels magical during New year')[-5:] if w not in stop]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-kfaIHeMrHq",
        "outputId": "053508bd-3ca9-4cdc-f5dc-6cd6252d08d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Its', 'a', 'wonderful', 'day']\n",
            "['beauti', 'day']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feel', 'magic', 'dure', 'new', 'year']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Classification Using Logistic Regression"
      ],
      "metadata": {
        "id": "nTyPRM-_M1YD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(\n",
        "                            strip_accents = None, \n",
        "                            lowercase=False, preprocessor=None, tokenizer= tokenizer_porter,\n",
        "                            use_idf = True, norm='l2',\n",
        "                            smooth_idf= True\n",
        ")\n",
        "y = df.sentiment.values\n",
        "X = tfidf.fit_transform(df.review)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1,test_size=0.5,shuffle=False)\n",
        "clf = LogisticRegressionCV( cv=5, scoring = 'accuracy', random_state=0,n_jobs=-1,verbose = 3, max_iter = 300).fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzwOQuOsM57V",
        "outputId": "0dd37df3-67a8-4be3-b459-78e60bb9303f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.0min finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation"
      ],
      "metadata": {
        "id": "s8AsPiRgNAG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DREsOFc2M-xa",
        "outputId": "5e289763-2b50-4179-af3b-97fa70c488f9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.89472"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}